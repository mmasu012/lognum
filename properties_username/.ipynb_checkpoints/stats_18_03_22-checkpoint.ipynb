{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1894,
   "id": "707085b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/masud/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os.path import exists\n",
    "import re\n",
    "import wordsegment as ws\n",
    "from wordsegment import load, segment, clean\n",
    "load()\n",
    "import enchant\n",
    "english_dict = enchant.Dict(\"en_US\")\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_found = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1895,
   "id": "6670aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slangification_map={\n",
    "    '0': ['o'],\n",
    "    '1': ['i', 'l', 't'],\n",
    "    '2': ['z'],\n",
    "    '3': ['e', 's'],\n",
    "    '4': ['a', 'r'],\n",
    "    '5': ['s'],\n",
    "    '6': ['g'],\n",
    "    '7': ['t'],\n",
    "    '8': ['b'],\n",
    "    '9': ['g'],\n",
    "    '@': ['a'],\n",
    "    '!': ['i'],\n",
    "    '$': ['s'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1896,
   "id": "773e6dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '3', '4', '1', '1', '2', '$']"
      ]
     },
     "execution_count": 1896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[' + ''.join(slangification_map.keys()) + ']{1,1}', '[23ma[4s1ud12$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1897,
   "id": "505e4683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "['m.lasudlover', 'm.iasudlover', 'm.tasudlovsr', 'm.lasudlovsr', 'm.tasudlover', 'm.iasudlovsr']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Considering all slangifying chars will be slangified all at once, if found\n",
    "def slangify(username):\n",
    "    \n",
    "    slang_chars = re.findall('[' + ''.join(slangification_map.keys()) + ']{1,1}', username)\n",
    "    # print(slang_chars)\n",
    "    \n",
    "    \n",
    "    if len(slang_chars) > 0:\n",
    "        \n",
    "        transformed_usernames=[username]\n",
    "        for char in slang_chars:\n",
    "            \n",
    "            replace_vals = slangification_map[char]\n",
    "            temp = []\n",
    "            \n",
    "            for r_v in replace_vals:\n",
    "                for t_u in transformed_usernames:\n",
    "                    temp.append( t_u.replace(char, r_v) )\n",
    "            \n",
    "            transformed_usernames = temp\n",
    "        \n",
    "        \n",
    "        transformed_usernames = list(set(transformed_usernames))\n",
    "#         print('Transformation is finished', transformed_usernames)\n",
    "        \n",
    "        return transformed_usernames\n",
    "    \n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "print('test')\n",
    "print(slangify('m.1@$udlov3r'))\n",
    "print(slangify('masud'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1898,
   "id": "249df569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, ['the', 'god'], ['vanda', 'the', 'god'])"
      ]
     },
     "execution_count": 1898,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract meaningful words\n",
    "def extract_meaningful_chunks(username):\n",
    "    \n",
    "    chunks_based_on_word_segmentation = segment(username)\n",
    "#     print(username, len(username))\n",
    "#     print(chunks_based_on_word_segmentation)\n",
    "    \n",
    "    meaningful_chunks = []\n",
    "    sentiment_score = 0\n",
    "    length = 0\n",
    "    for chunk in chunks_based_on_word_segmentation:        \n",
    "        \n",
    "        if len(chunk) >= 3: # at least 3 letters\n",
    "            \n",
    "#             synsets = wordnet.synsets(chunk)\n",
    "#             if synsets:\n",
    "#                 synset = synsets[0]\n",
    "             if english_dict.check(chunk):\n",
    "#                 print(chunk, 'yo')\n",
    "                meaningful_chunks.append(chunk)\n",
    "                length += len(chunk)\n",
    "    \n",
    "#     print(length, meaningful_chunks, chunks_based_on_word_segmentation)\n",
    "    return length, meaningful_chunks, chunks_based_on_word_segmentation\n",
    "\n",
    "# extract_meaningful_chunks('pttoym.iasudlov.erlov.er')\n",
    "extract_meaningful_chunks('vandathegod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1899,
   "id": "d7820151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('m.lasudlover', ['lover'], ['mla', 'sud', 'lover'])"
      ]
     },
     "execution_count": 1899,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def extract_max_meaningful_chunkification(slangifications):\n",
    "    \n",
    "    max_length = 0\n",
    "    max_meaningful_chunkification = []\n",
    "    max_slangification = ''\n",
    "    max_chunks_based_on_word_segmentation = []\n",
    "    for slangification in slangifications:\n",
    "\n",
    "        length, meaningful_chunks, chunks_based_on_word_segmentation = extract_meaningful_chunks(slangification)\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            max_meaningful_chunkification = meaningful_chunks\n",
    "            max_slangification = slangification\n",
    "            max_chunks_based_on_word_segmentation = chunks_based_on_word_segmentation\n",
    "\n",
    "#     print(max_slangification, max_meaningful_chunkification, max_chunks_based_on_word_segmentation)\n",
    "    return max_slangification, max_meaningful_chunkification, max_chunks_based_on_word_segmentation\n",
    "\n",
    "extract_max_meaningful_chunkification(['m.lasudlover', 'm.iasudlover', 'm.tasudlovsr', 'm.lasudlovsr', 'm.tasudlover', 'm.iasudlovsr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1900,
   "id": "20dcc209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 20)"
      ]
     },
     "execution_count": 1900,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "# str1 = \"lover\"\n",
    "# str2 = \"lovetoym.iasudlov.erlov.er\"\n",
    "def extract_position(str1, str2, start_pos_str2):\n",
    "    \n",
    "    m = len(str1)\n",
    "    n = len(str2)\n",
    " \n",
    "    j = 0    # Index of str1\n",
    "#     i = 0    # Index of str2\n",
    "    i = start_pos_str2\n",
    " \n",
    "    # Traverse both str1 and str2\n",
    "    # Compare current character of str2 with\n",
    "    # first unmatched character of str1\n",
    "    # If matched, then move ahead in str1\n",
    " \n",
    "    while j < m and i < n:\n",
    "        if str1[j] == str2[i]:\n",
    "            j = j+1\n",
    "        i = i + 1\n",
    " \n",
    "        \n",
    "    end = i\n",
    "    i = i - 1\n",
    "    j = m - 1\n",
    "\n",
    "    while j > -1 and i > start_pos_str2 - 1:\n",
    "        if str1[j] == str2[i]:\n",
    "            j = j-1\n",
    "        i = i - 1\n",
    "    i += 2\n",
    "    start = i\n",
    "\n",
    "    return start, end\n",
    "\n",
    "str1 = \"lover\"\n",
    "str2 = \"lovetoym.iasudlov.erlov.er\"\n",
    "extract_position(str1, str2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1901,
   "id": "63647ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 26)"
      ]
     },
     "execution_count": 1901,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# str1 = \"lover\"\n",
    "# str2 = \"lovetoym.iasudlov.erlov.er\"\n",
    "def is_subsequence(str1, str2, start_pos_str2):\n",
    "    \n",
    "    m = len(str1)\n",
    "    n = len(str2)\n",
    " \n",
    "    j = 0    # Index of str1\n",
    "#     i = 0    # Index of str2\n",
    "    i = start_pos_str2\n",
    " \n",
    "    # Traverse both str1 and str2\n",
    "    # Compare current character of str2 with\n",
    "    # first unmatched character of str1\n",
    "    # If matched, then move ahead in str1\n",
    " \n",
    "    while j < m and i < n:\n",
    "        if str1[j] == str2[i]:\n",
    "            j = j+1\n",
    "        i = i + 1\n",
    " \n",
    "    return j == m\n",
    "\n",
    "str1 = \"zlover\"\n",
    "str2 = \"lovetoym.iasudlov.erlov.er\"\n",
    "extract_position(str1, str2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1902,
   "id": "054302f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'toy',\n",
       "  'start': 2,\n",
       "  'end': 4,\n",
       "  'type': 'normal',\n",
       "  'withJunk': False,\n",
       "  'pos': [('going', 'VBG')]},\n",
       " {'word': 'lover',\n",
       "  'start': 12,\n",
       "  'end': 17,\n",
       "  'type': 'normal',\n",
       "  'withJunk': True,\n",
       "  'pos': [('going', 'VBG')]},\n",
       " {'word': 'lover',\n",
       "  'start': 18,\n",
       "  'end': 23,\n",
       "  'type': 'normal',\n",
       "  'withJunk': True,\n",
       "  'pos': [('going', 'VBG')]}]"
      ]
     },
     "execution_count": 1902,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perform_chunkification_mapping(max_meaningful_chunkification, max_slangification, \n",
    "                                   chunkification_type,\n",
    "                                  username) :\n",
    "    \n",
    "    chunkification_mapping = []    \n",
    "    index = 0\n",
    "    for meaningful_chunkification in max_meaningful_chunkification:            \n",
    "        \n",
    "        start, end = extract_position(meaningful_chunkification, max_slangification, index)\n",
    "        index = end\n",
    "        \n",
    "#         print(meaningful_chunkification, max_slangification)\n",
    "#         print(start, end)\n",
    "    \n",
    "        if chunkification_type == 'slangified':\n",
    "            if is_subsequence(meaningful_chunkification, username, 0):\n",
    "                continue\n",
    "\n",
    "        words_found =  words_found + ' '\n",
    "        \n",
    "        chunkification_mapping.append({\n",
    "            'word': meaningful_chunkification,\n",
    "            'start': start - 1,\n",
    "            'end': end - 1,\n",
    "            'type': chunkification_type,\n",
    "            'withJunk': (end - start + 1) > len(meaningful_chunkification),\n",
    "        })\n",
    "    \n",
    "#     print(chunkification_mapping)\n",
    "\n",
    "    return chunkification_mapping\n",
    "\n",
    "perform_chunkification_mapping(['toy', 'lover', 'lover'], 'pttoym.iasudlov.erlov.er', 'normal', 'pttoym.iasudlov.erlov.er')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1903,
   "id": "9fe72495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'toy', 'start': 2, 'end': 4, 'type': 'normal', 'withJunk': False}\n",
      "{'word': 'lover', 'start': 12, 'end': 17, 'type': 'normal', 'withJunk': True}\n",
      "{'word': 'lover', 'start': 18, 'end': 23, 'type': 'normal', 'withJunk': True}\n"
     ]
    }
   ],
   "source": [
    "def printMapping(mapping):\n",
    "    \n",
    "    for elem in mapping:\n",
    "        print(elem)\n",
    "\n",
    "printMapping([{'word': 'toy', 'start': 2, 'end': 4, 'type': 'normal', 'withJunk': False},\n",
    " {'word': 'lover', 'start': 12, 'end': 17, 'type': 'normal', 'withJunk': True},\n",
    " {'word': 'lover', 'start': 18, 'end': 23, 'type': 'normal', 'withJunk': True}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1904,
   "id": "9e30c39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True]"
      ]
     },
     "execution_count": 1904,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_used_unused_mapping(username, mapping):\n",
    "    \n",
    "    used_flag = [False] * len(username)\n",
    "    for elem in mapping:\n",
    "        \n",
    "        for index in range(elem['start'], elem['end'] + 1):\n",
    "            used_flag[index] = True\n",
    "            \n",
    "#     print(used_flag)\n",
    "    return used_flag\n",
    "\n",
    "get_used_unused_mapping('pt7oym.1@$udlov.3rlov.3r', [{'word': 'toy', 'start': 2, 'end': 4, 'type': 'normal', 'withJunk': False},\n",
    "{'word': 'lover', 'start': 12, 'end': 17, 'type': 'normal', 'withJunk': True},\n",
    "{'word': 'lover', 'start': 18, 'end': 23, 'type': 'normal', 'withJunk': True}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1905,
   "id": "f1639187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (5, 11)]"
      ]
     },
     "execution_count": 1905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_unused_blocks(used_unused_mapping):\n",
    "    \n",
    "    index = 0\n",
    "    start = -1\n",
    "    unused_blocks = []\n",
    "    while index < len(used_unused_mapping):\n",
    "        \n",
    "        \n",
    "        if used_unused_mapping[index] == False:\n",
    "            if start == -1:\n",
    "                start = index\n",
    "        else:\n",
    "            if start != -1:\n",
    "                end = index - 1\n",
    "                unused_blocks.append((start, end))\n",
    "                start = -1\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    if start != -1:\n",
    "        unused_blocks.append((start, len(used_unused_mapping) - 1))\n",
    "    \n",
    "#     print(unused_blocks)\n",
    "\n",
    "    return unused_blocks\n",
    "\n",
    "extract_unused_blocks([False, False, True, True, True, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1906,
   "id": "7e5d1934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '2', 'start': 5, 'end': 5, 'type': 'digits', 'more_than_one': False},\n",
       " {'word': '123',\n",
       "  'start': 9,\n",
       "  'end': 11,\n",
       "  'type': 'digits',\n",
       "  'more_than_one': True},\n",
       " {'word': '123',\n",
       "  'start': 15,\n",
       "  'end': 17,\n",
       "  'type': 'digits',\n",
       "  'more_than_one': True},\n",
       " {'word': '3',\n",
       "  'start': 19,\n",
       "  'end': 19,\n",
       "  'type': 'digits',\n",
       "  'more_than_one': False}]"
      ]
     },
     "execution_count": 1906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_digit_chunkification(blocks, username, chunkification_type):\n",
    "    \n",
    "    mapping = []\n",
    "    for block in blocks:\n",
    "        start, end = block\n",
    "#         print(start, end)\n",
    "        string_block = username[start: end + 1]\n",
    "        digit_block_list = re.findall('\\d+', string_block)\n",
    "        index = 0 # starting index\n",
    "        for digit_block in digit_block_list:\n",
    "            \n",
    "            start_index_in_block = string_block.find(digit_block, index)\n",
    "            end_index_in_block = start_index_in_block + len(digit_block) - 1\n",
    "            index = end_index_in_block + 1\n",
    "            \n",
    "            start_index_in_username = start + start_index_in_block\n",
    "            end_index_in_username = start + end_index_in_block\n",
    "            \n",
    "            \n",
    "#             print(start_index_in_block, end_index_in_block, index)\n",
    "            mapping.append({\n",
    "                'word': digit_block,\n",
    "                'start': start_index_in_username,\n",
    "                'end': end_index_in_username,\n",
    "                'type': chunkification_type,\n",
    "                'more_than_one': (end_index_in_username - start_index_in_username + 1) > 1,\n",
    "            })\n",
    "    \n",
    "#     print(mapping)\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "extract_digit_chunkification([(0, 1), (5, 11)], 'pt7oym.1@$udlov.3rlov.3r', 'digits')\n",
    "extract_digit_chunkification([(5, 19)], 'karma2mas123sud123f3lover3r', 'digits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1907,
   "id": "527e7975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'masud', 'start': 1, 'end': 5, 'type': 'name'},\n",
       " {'word': 'd', 'start': 5, 'end': 5, 'type': 'name'},\n",
       " {'word': 'rr', 'start': 10, 'end': 11, 'type': 'name'}]"
      ]
     },
     "execution_count": 1907,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_name_chunkification(blocks, username, chunkification_type):\n",
    "    \n",
    "    mapping = []\n",
    "    for block in blocks:\n",
    "        start, end = block\n",
    "#         print(start, end)\n",
    "        string_block = username[start: end + 1]\n",
    "        name_block_list = re.findall('[a-z]+', string_block)\n",
    "        index = 0 # starting index\n",
    "        for name_block in name_block_list:\n",
    "            \n",
    "            start_index_in_block = string_block.find(name_block, index)\n",
    "            end_index_in_block = start_index_in_block + len(name_block) - 1\n",
    "            index = end_index_in_block + 1\n",
    "            \n",
    "            start_index_in_username = start + start_index_in_block\n",
    "            end_index_in_username = start + end_index_in_block\n",
    "            \n",
    "            \n",
    "#             print(start_index_in_block, end_index_in_block, index)\n",
    "            mapping.append({\n",
    "                'word': name_block,\n",
    "                'start': start_index_in_username,\n",
    "                'end': end_index_in_username,\n",
    "                'type': chunkification_type,\n",
    "            })\n",
    "    \n",
    "#     print(mapping)\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "extract_name_chunkification([(0, 7), (5, 11)], '3masud..@4rr', 'name')\n",
    "# extract_name_chunkification([(5, 19)], '', 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1908,
   "id": "7d27da62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'fight',\n",
       "  'start': 3,\n",
       "  'end': 7,\n",
       "  'type': 'normal',\n",
       "  'withJunk': False,\n",
       "  'pos': [('going', 'VBG')]},\n",
       " {'word': 'for',\n",
       "  'start': 9,\n",
       "  'end': 11,\n",
       "  'type': 'normal',\n",
       "  'withJunk': False,\n",
       "  'pos': [('going', 'VBG')]},\n",
       " {'word': 'talk',\n",
       "  'start': 13,\n",
       "  'end': 17,\n",
       "  'type': 'normal',\n",
       "  'withJunk': True,\n",
       "  'pos': [('going', 'VBG')]},\n",
       " {'word': 'and',\n",
       "  'start': 19,\n",
       "  'end': 21,\n",
       "  'type': 'normal',\n",
       "  'withJunk': False,\n",
       "  'pos': [('going', 'VBG')]},\n",
       " {'word': 'god',\n",
       "  'start': 27,\n",
       "  'end': 29,\n",
       "  'type': 'normal',\n",
       "  'withJunk': False,\n",
       "  'pos': [('going', 'VBG')]}]"
      ]
     },
     "execution_count": 1908,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_word_chunkification(blocks, username, chunkification_type):\n",
    "    \n",
    "    mapping = []\n",
    "    for block in blocks:\n",
    "        start, end = block\n",
    "#         print(start, end)\n",
    "        string_block = username[start: end + 1]\n",
    "        length, meaningful_chunks, chunks_based_on_word_segmentation = extract_meaningful_chunks(string_block)\n",
    "        if len(meaningful_chunks) > 0:\n",
    "            word_mapping = perform_chunkification_mapping(meaningful_chunks,\\\n",
    "                                                    string_block, \\\n",
    "                                                    'normal',\\\n",
    "                                                    username)\n",
    "            \n",
    "            for elem in word_mapping:\n",
    "                elem['start'] += start\n",
    "                elem['end'] += start\n",
    "            \n",
    "            mapping += word_mapping\n",
    "#     print()\n",
    "#     printMapping(mapping)\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "extract_word_chunkification([(3,18), (19,29)], '123fight.for@tal.kvanda.thegod', 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1909,
   "id": "efaf459a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '$@', 'start': 0, 'end': 1, 'type': 'symbol', 'more_than_one': True},\n",
       " {'word': '&', 'start': 5, 'end': 5, 'type': 'symbol', 'more_than_one': False}]"
      ]
     },
     "execution_count": 1909,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_symbol_chunkification(blocks, username, chunkification_type):\n",
    "    \n",
    "    mapping = []\n",
    "    for block in blocks:\n",
    "        start, end = block\n",
    "#         print(start, end)\n",
    "        string_block = username[start: end + 1]\n",
    "        \n",
    "        mapping.append({\n",
    "            'word': string_block,\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'type': chunkification_type,\n",
    "            'more_than_one': (end - start + 1) > 1,\n",
    "        })\n",
    "    \n",
    "#     print(mapping)\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "get_symbol_chunkification([(0, 1), (5, 5)], '$@abc&99', 'symbol')\n",
    "# get_symbol_chunkification([(5, 19)], 'karma2mas123sud123f3lover3r', 'digits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1910,
   "id": "bd8fd19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_structure(chunkification_mapping):\n",
    "    \n",
    "#     for mapping in chunkification_mapping:\n",
    "#         print(mapping)\n",
    "    \n",
    "#     print()\n",
    "#     print()\n",
    "\n",
    "    sorted_chunkification_mapping = sorted(chunkification_mapping, key=lambda d: d['start']) \n",
    "    structure = []\n",
    "    for mapping in sorted_chunkification_mapping:\n",
    "        \n",
    "        if mapping['type'] == 'symbol':\n",
    "            structure.append('Sym')\n",
    "        elif mapping['type'] == 'name':\n",
    "            structure.append('Nam')\n",
    "        elif mapping['type'] == 'digits':\n",
    "            structure.append('Dig')\n",
    "        elif mapping['type'] == 'normal':\n",
    "            if mapping['withJunk']:\n",
    "                structure.append('WrdS')\n",
    "            else:\n",
    "                structure.append('Wrd')\n",
    "        elif mapping['type'] == 'slangified':\n",
    "            if mapping['withJunk']:\n",
    "                structure.append('SlgS')\n",
    "            else:\n",
    "                structure.append('Slg')\n",
    "    \n",
    "    return structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1911,
   "id": "2cd28209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nam-WrdS-Dig-Wrd'"
      ]
     },
     "execution_count": 1911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_username_structure(username):\n",
    "\n",
    "    # print(\"slangification\")\n",
    "    # print()\n",
    "    chunks = slangify(username)\n",
    "    chunkification_type = 'slangified'\n",
    "    max_slangification, max_meaningful_chunkification,\\\n",
    "        max_chunks_based_on_word_segmentation = extract_max_meaningful_chunkification(chunks)\n",
    "\n",
    "    # chunks = slangify('masudh@ppy')\n",
    "    chunkification_mapping = []\n",
    "    slang_chunkification_mapping = []\n",
    "    if len(max_meaningful_chunkification) > 0:\n",
    "        slang_chunkification_mapping = perform_chunkification_mapping(max_meaningful_chunkification,\\\n",
    "                                                                max_slangification, \\\n",
    "                                                                chunkification_type, \\\n",
    "                                                                username)\n",
    "\n",
    "    chunkification_mapping += slang_chunkification_mapping\n",
    "    # print()\n",
    "    # print(\"slangification ouput\")\n",
    "    # printMapping(chunkification_mapping)\n",
    "\n",
    "    used_unused_mapping = get_used_unused_mapping(username, chunkification_mapping)\n",
    "    # print()\n",
    "\n",
    "    # print(username)\n",
    "    unused_blocks = extract_unused_blocks(used_unused_mapping)\n",
    "    # print()\n",
    "\n",
    "\n",
    "    # print(\"digits\")\n",
    "    # print()\n",
    "    digit_chunkification_mapping = extract_digit_chunkification(unused_blocks, \\\n",
    "                                                                username, \\\n",
    "                                                               'digits')\n",
    "\n",
    "    chunkification_mapping += digit_chunkification_mapping\n",
    "    # print()\n",
    "    # print(\"digits ouput\")\n",
    "    # printMapping(chunkification_mapping)\n",
    "\n",
    "    used_unused_mapping = get_used_unused_mapping(username, chunkification_mapping)\n",
    "    # print()\n",
    "\n",
    "    # print(username)\n",
    "    unused_blocks = extract_unused_blocks(used_unused_mapping)\n",
    "    # print()\n",
    "\n",
    "    # print(\"words\")\n",
    "    # print()\n",
    "\n",
    "    word_chunkification_mapping = extract_word_chunkification(unused_blocks, \\\n",
    "                                                                username, \\\n",
    "                                                               'normal')\n",
    "\n",
    "    chunkification_mapping += word_chunkification_mapping\n",
    "    # print()\n",
    "    # print(\"words ouput\")\n",
    "    # printMapping(chunkification_mapping)\n",
    "\n",
    "    used_unused_mapping = get_used_unused_mapping(username, chunkification_mapping)\n",
    "    # print()\n",
    "\n",
    "    # print(username)\n",
    "    unused_blocks = extract_unused_blocks(used_unused_mapping)\n",
    "    # print()\n",
    "\n",
    "    # print(\"names\")\n",
    "    # print()\n",
    "\n",
    "    name_chunkification_mapping = extract_name_chunkification(unused_blocks, \\\n",
    "                                                                username, \\\n",
    "                                                               'name')\n",
    "\n",
    "    chunkification_mapping += name_chunkification_mapping\n",
    "    # print()\n",
    "    # print(\"names ouput\")\n",
    "    # printMapping(chunkification_mapping)\n",
    "\n",
    "    used_unused_mapping = get_used_unused_mapping(username, chunkification_mapping)\n",
    "    # print()\n",
    "\n",
    "    # print(username)\n",
    "    unused_blocks = extract_unused_blocks(used_unused_mapping)\n",
    "    # print()\n",
    "\n",
    "    # print(\"symbol\")\n",
    "    # print()\n",
    "\n",
    "    symbol_chunkification_mapping = get_symbol_chunkification(unused_blocks, username, 'symbol')\n",
    "\n",
    "    chunkification_mapping += symbol_chunkification_mapping\n",
    "    # print()\n",
    "    # print(\"symbols ouput\")\n",
    "    # printMapping(chunkification_mapping)\n",
    "\n",
    "    used_unused_mapping = get_used_unused_mapping(username, chunkification_mapping)\n",
    "    # print()\n",
    "\n",
    "    # print(username)\n",
    "    unused_blocks = extract_unused_blocks(used_unused_mapping)\n",
    "    # print(unused_blocks)\n",
    "\n",
    "    structure = generate_structure(chunkification_mapping)\n",
    "    # print()\n",
    "#     print(username)\n",
    "#     print('-'.join(structure))\n",
    "    return '-'.join(structure)\n",
    "\n",
    "# username = 'masud@ab'\n",
    "username = 'vandath.e90god'\n",
    "# username = 'v.i.r.u.s'\n",
    "# username = '7oym.1@$udlov.3rlov.3r456'\n",
    "get_username_structure(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1912,
   "id": "ac712573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethical_hacker\n",
      "Empty DataFrame\n",
      "Columns: [index, name]\n",
      "Index: []\n",
      "index    0\n",
      "name     0\n",
      "dtype: int64\n",
      "ethical_hacker\n",
      "total user count 2262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forum_list = [\n",
    "        \n",
    "        'ethical_hacker',\n",
    "#         'hack_this_site',\n",
    "#         'mpgh',\n",
    "#         'security_stack_exchange',\n",
    "#         'garage4hackers',\n",
    "#         'wilderssecurity',\n",
    "#         'offensive_community',\n",
    "#         'hack_forums',\n",
    "#         'raidforums',\n",
    "        # 'google_plus',\n",
    "#         'facebook',\n",
    "        # 'twitter',         \n",
    "]\n",
    "# forum_list\n",
    "\n",
    "\n",
    "result_dict = {\n",
    "        'id': [],\n",
    "        'forum_name': [], \n",
    "        'username': [],\n",
    "        'name_struct': [],\n",
    "}\n",
    "index = 0\n",
    "\n",
    "for forum_name in forum_list:\n",
    "    \n",
    "    data_dir = 'processed/'\n",
    "    file_location = data_dir + forum_name + '_usernames.csv'\n",
    "\n",
    "    data = pd.read_csv(file_location)\n",
    "    \n",
    "    print(forum_name)\n",
    "    \n",
    "    print(data[data.duplicated(keep=False)])\n",
    "    print(data.isnull().sum())\n",
    "#     print(data.columns)\n",
    "\n",
    "    data_map = data.to_dict('records')\n",
    "    print(forum_name)\n",
    "    print('total user count', data.shape[0])\n",
    "#     print()\n",
    "    print()\n",
    "    \n",
    "    for iteration, record in enumerate(data_map):\n",
    "        \n",
    "        username = record['name'].lower()\n",
    "        name_struct = get_username_structure(username)\n",
    "        \n",
    "        index += 1\n",
    "        result_dict['id'].append(index)\n",
    "        result_dict['forum_name'].append(forum_name)\n",
    "        result_dict['username'].append(username)\n",
    "        result_dict['name_struct'].append(name_struct)\n",
    "    \n",
    "    \n",
    "result_data = pd.DataFrame.from_dict(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1913,
   "id": "d923803c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ethical_hacker']\n"
     ]
    }
   ],
   "source": [
    "print(result_data['forum_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1914,
   "id": "579e0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_data.to_csv('name_structs_fb_2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1915,
   "id": "7a6db634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['skymaster']"
      ]
     },
     "execution_count": 1915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # check in two dataset\n",
    "print(english_dict.check('skymaster'))\n",
    "# l = [False] * 3\n",
    "# l\n",
    "segment('skymaster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1916,
   "id": "52a3826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets = wordnet.synsets('skymaster')\n",
    "if synsets:\n",
    "    synset = synsets[0]\n",
    "    synsets[0].pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1917,
   "id": "d049c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 'mas123df124'\n",
    "# a.find('12', 5)\n",
    "\n",
    "# re.findall('[a-z]+', 'abc.rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ad39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1918,
   "id": "c9374535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = nltk.pos_tag(['excellent'])\n",
    "  \n",
    "# # ans returns a list of tuple\n",
    "# val = ans[0][1]\n",
    "  \n",
    "# # checking if it is a noun or not\n",
    "# if(val == 'NN' or val == 'NNS' or val == 'NNPS' or val == 'NNP'):\n",
    "#     print( \" is a noun.\")\n",
    "# else:\n",
    "#     print( \" is not a noun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf41c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
